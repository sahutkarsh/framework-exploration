{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100 #Termination Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.1 # Learning Rate\n",
    "dr = 0.99 # Discount Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ r_f = r_{min} + (r_{max} - r_{min}) * e^{-\\lambda t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exploration_rate = 1\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_exp_rate = 0.01\n",
    "max_exp_rate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Equation: $ q_f(s,a) = (1 - \\alpha) * q_i(s,a) + \\alpha * (R_{t+1} + \\gamma * max(s',a'))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check Average Reward of past n episodes\n",
    "def check_progress(episode, rewards, n):\n",
    "    if ((episode + 1) % n != 0):\n",
    "        return\n",
    "    else:\n",
    "        avg_reward = np.mean(rewards[(episode - n + 1):])\n",
    "        print('Episode: {} Average Reward: {}'.format(episode + 1, avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000 Average Reward: 0.039\n",
      "Episode: 2000 Average Reward: 0.235\n",
      "Episode: 3000 Average Reward: 0.385\n",
      "Episode: 4000 Average Reward: 0.542\n",
      "Episode: 5000 Average Reward: 0.632\n",
      "Episode: 6000 Average Reward: 0.632\n",
      "Episode: 7000 Average Reward: 0.687\n",
      "Episode: 8000 Average Reward: 0.666\n",
      "Episode: 9000 Average Reward: 0.663\n",
      "Episode: 10000 Average Reward: 0.675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False # Episode Termination Switch\n",
    "    episode_reward = 0 # Overall reward for the Episode\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        threshold = random.uniform(0,1)\n",
    "        if (threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, :]) # Exploit\n",
    "        else:\n",
    "            action = env.action_space.sample() # Explore\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action) # Take Step with selected Action\n",
    "        \n",
    "        # Q-Table Update\n",
    "        q_table[state, action] = (1-lr) * q_table[state, action] + lr * (reward + dr * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state # Update State\n",
    "        episode_reward += reward # Accumulate reward from each step\n",
    "        \n",
    "        if (done):\n",
    "            break # Terminate\n",
    "        \n",
    "    #Exploration Rate Decay\n",
    "    exploration_rate = min_exp_rate + (max_exp_rate - min_exp_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "    check_progress(episode, rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53505349, 0.50484086, 0.50474339, 0.49548158],\n",
       "       [0.19554661, 0.38072052, 0.23655282, 0.45402011],\n",
       "       [0.36756438, 0.24424303, 0.28829625, 0.27052928],\n",
       "       [0.15391262, 0.03965372, 0.03802641, 0.07170435],\n",
       "       [0.55950198, 0.37385618, 0.38738822, 0.28279125],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.19746888, 0.10208733, 0.14211531, 0.12840029],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.41206145, 0.44186514, 0.32148788, 0.60543911],\n",
       "       [0.3347863 , 0.661227  , 0.47620774, 0.33739398],\n",
       "       [0.58843177, 0.379851  , 0.28765794, 0.30881125],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.43588327, 0.45487625, 0.78966314, 0.57921869],\n",
       "       [0.66852449, 0.90658595, 0.72376885, 0.73912602],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Watching Agent Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_episodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Goal Reached!\n"
     ]
    }
   ],
   "source": [
    "for episode in range(n_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print('Episode:', (episode+1))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if(done):\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if (reward == 1):\n",
    "                print('Goal Reached!')\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('You Fell in the Hole!')\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "22dbd9d79f8243958c419bdc2d093050": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
