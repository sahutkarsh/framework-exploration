{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100 #Termination Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.1 # Learning Rate\n",
    "dr = 0.99 # Discount Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ r_f = r_{min} + (r_{max} - r_{min}) * e^{-\\lambda t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exploration_rate = 1\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_exp_rate = 0.01\n",
    "max_exp_rate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Equation: $ q_f(s,a) = (1 - \\alpha) * q_i(s,a) + \\alpha * (R_{t+1} + \\gamma * max(s',a'))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check Average Reward of past n episodes\n",
    "def check_progress(episode, rewards, n):\n",
    "    if ((episode + 1) % n != 0):\n",
    "        return\n",
    "    else:\n",
    "        avg_reward = np.mean(rewards[(episode - n + 1):])\n",
    "        print('Episode: {} Average Reward: {}'.format(episode + 1, avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000 Average Reward: 0.035\n",
      "Episode: 2000 Average Reward: 0.195\n",
      "Episode: 3000 Average Reward: 0.393\n",
      "Episode: 4000 Average Reward: 0.566\n",
      "Episode: 5000 Average Reward: 0.61\n",
      "Episode: 6000 Average Reward: 0.637\n",
      "Episode: 7000 Average Reward: 0.696\n",
      "Episode: 8000 Average Reward: 0.681\n",
      "Episode: 9000 Average Reward: 0.675\n",
      "Episode: 10000 Average Reward: 0.655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False # Episode Termination Switch\n",
    "    episode_reward = 0 # Overall reward for the Episode\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        threshold = random.uniform(0,1)\n",
    "        if (threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, :]) # Exploit\n",
    "        else:\n",
    "            action = env.action_space.sample() # Explore\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action) # Take Step with selected Action\n",
    "        \n",
    "        # Q-Table Update\n",
    "        q_table[state, action] = (1-lr) * q_table[state, action] + lr * (reward + dr * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state # Update State\n",
    "        episode_reward += reward # Accumulate reward from each step\n",
    "        \n",
    "        if (done):\n",
    "            break # Terminate\n",
    "        \n",
    "    #Exploration Rate Decay\n",
    "    exploration_rate = min_exp_rate + (max_exp_rate - min_exp_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "    check_progress(episode, rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47020199, 0.43860707, 0.44000943, 0.43793491],\n",
       "       [0.32024148, 0.28840432, 0.19553546, 0.36756947],\n",
       "       [0.29554924, 0.27063586, 0.25113214, 0.26389693],\n",
       "       [0.06066394, 0.10990785, 0.03796044, 0.06608058],\n",
       "       [0.50418147, 0.317521  , 0.4020053 , 0.37648037],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.16434316, 0.13014516, 0.23939376, 0.05636354],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.42212087, 0.41614798, 0.40899276, 0.53229378],\n",
       "       [0.49152029, 0.59181527, 0.38904501, 0.49640466],\n",
       "       [0.539314  , 0.4053943 , 0.46114049, 0.29184201],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.47905465, 0.56431767, 0.66502172, 0.54570567],\n",
       "       [0.70245215, 0.80056656, 0.72424465, 0.71957296],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "b1f927d8f69e454eb3a1223ffc7f3194": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
